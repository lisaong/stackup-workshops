{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/stackup-workshops/blob/master/text-similarity/text_processing101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR9eZTvhAMD9",
        "colab_type": "text"
      },
      "source": [
        "# Text Processing 101 Demo Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1574A2QSDjvH",
        "colab_type": "text"
      },
      "source": [
        "## Part 1 - Text Processing\n",
        "\n",
        "1. Load a language model\n",
        "2. Tokenise the text\n",
        "3. Clean the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMrcwUp-DgYF",
        "colab_type": "text"
      },
      "source": [
        "### Language models\n",
        "\n",
        "Install spacy language models for:\n",
        "- English\n",
        "- Chinese\n",
        "\n",
        "Language models come in different sizes, larger models include more words. Each language is trained on a different corpus.\n",
        "\n",
        "For example:\n",
        "\n",
        "| Model | English (Common Crawl) | Chinese (OntoNotes) |\n",
        "|--|--|--|\n",
        "| Small | 11MB | 45MB|\n",
        "| Medium | 45MB | 75MB |\n",
        "| Large | 746MB | 575MB |\n",
        "\n",
        "Full list here: https://spacy.io/usage/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxLd2U-mEOfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "cf55af69-5ed8-460d-de34-95954f4ed307"
      },
      "source": [
        "# Instal spacy 2.3 so that we can use Chinese models\n",
        "!pip install spacy==2.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/c7/e66e2af1cfa418c3a3917c116c4e00ccffa546f18f59e6acd7953d833c5c/spacy-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (10.0MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (47.3.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3) (1.0.2)\n",
            "Collecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3) (1.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3) (3.1.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.0 thinc-7.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag4fSPT_0yqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "664f9a18-53af-4914-e678-fff47f612294"
      },
      "source": [
        "# We will load the medium-sized model, which will include word vectors\n",
        "# word vectors are useful in Part 2 for finding the meaning of a text sequence\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.3.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.0/en_core_web_md-2.3.0.tar.gz (50.8MB)\n",
            "\u001b[K     |████████████████████████████████| 50.8MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (47.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (7.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.6.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.0-cp36-none-any.whl size=50921515 sha256=1bd7253371931e433952d4f8ce5312c03c40b1756f69d0265310aaa3fac5a6c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sg10o4yh/wheels/0d/1f/15/ea2b10dec65de31d56a2b3f383f4e21e1e6358f27877853fbb\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgeTgri2D2Ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "7bf32ae0-1f96-451e-c166-c1fef8869867"
      },
      "source": [
        "# do the same for the Chinese model\n",
        "!python -m spacy download zh_core_web_md"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zh_core_web_md==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-2.3.1/zh_core_web_md-2.3.1.tar.gz (78.9MB)\n",
            "\u001b[K     |████████████████████████████████| 78.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from zh_core_web_md==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from zh_core_web_md==2.3.1) (0.42.1)\n",
            "Collecting pkuseg>=0.0.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/3a/090a533c7f0682d653633cfd2d33e9aab3e671379fb199aeb7fa9bd3c34a/pkuseg-0.0.25.tar.gz (48.8MB)\n",
            "\u001b[K     |████████████████████████████████| 48.8MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (47.3.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pkuseg>=0.0.22->zh_core_web_md==2.3.1) (0.29.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->zh_core_web_md==2.3.1) (3.1.0)\n",
            "Building wheels for collected packages: zh-core-web-md, pkuseg\n",
            "  Building wheel for zh-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zh-core-web-md: filename=zh_core_web_md-2.3.1-cp36-none-any.whl size=78733266 sha256=d00fc1ad3a33d00bdc023fda21a72a0538abd0648369e2b7c920fe620c52cfe9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-67x68dz0/wheels/8a/38/cf/bde95f3f103e5a7ba7e719f318cb3933577cfe3e3f19eab222\n",
            "  Building wheel for pkuseg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pkuseg: filename=pkuseg-0.0.25-cp36-cp36m-linux_x86_64.whl size=50131900 sha256=786763df5fd94dc4a15beeceef37454298cc64247d377e8d709bce54d259cc67\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-67x68dz0/wheels/7b/e1/23/a7ad8a65e052b7961808cc7d17236f6ba92fc451327f13f3f9\n",
            "Successfully built zh-core-web-md pkuseg\n",
            "Installing collected packages: pkuseg, zh-core-web-md\n",
            "Successfully installed pkuseg-0.0.25 zh-core-web-md-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('zh_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XUgjBmnD-hq",
        "colab_type": "text"
      },
      "source": [
        "### Restart the Runtime\n",
        "In order for the language models to load, we need to restart the Colab runtime.\n",
        "\n",
        "Runtime -> Restart Runtime\n",
        "\n",
        "This reuses the same cloud machine, but restarts Python so that it can find the new models.\n",
        "\n",
        "Otherwise you get this error: `OSError: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPad1uZJEF5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOSdQSJwGvXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "641c0ff4-1f26-47d8-eab0-a4364623f1cb"
      },
      "source": [
        "nlp_en = spacy.load(\"en_core_web_md\")\n",
        "nlp_zh = spacy.load(\"zh_core_web_md\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.919 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpWQyRQkG8hZ",
        "colab_type": "text"
      },
      "source": [
        "### Tokenise some text\n",
        "\n",
        "We'll try both English and Chinese text from news articles.\n",
        "\n",
        "Note that the language models are not interchangeable (different vocabulary, tokenisation, linguistic features)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqpYCZWxG_6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a4b45cf-fa1f-4172-a97a-3b3b7bf5324a"
      },
      "source": [
        "# Taken from: https://www.straitstimes.com/singapore/jobs-for-many-singaporeans-in-growing-infocomm-sector\n",
        "text_en = \"\"\"\n",
        "Jobs are available for Singaporeans in the infocommunications space over the next three years, with the Republic facing a huge shortage of professionals, said Minister-in-charge of the Smart Nation Initiative Vivian Balakrishnan.\n",
        "\n",
        "Singaporeans can secure these well-paying jobs if they are prepared to train and reskill, said Dr Balakrishnan, who is also Foreign Minister, in an interview with The Straits Times.\n",
        "\n",
        "He said digitalisation and disruption had already taken root when the current crisis struck. Covid-19 accelerated these trends.\n",
        "\n",
        "While Singapore's immediate priority is to save jobs, that is not enough because \"jobs are going to change\", he added.\n",
        "\"\"\"\n",
        "\n",
        "doc_en = nlp_en(text_en)\n",
        "\n",
        "[t for t in doc_en]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[,\n",
              " Jobs,\n",
              " are,\n",
              " available,\n",
              " for,\n",
              " Singaporeans,\n",
              " in,\n",
              " the,\n",
              " infocommunications,\n",
              " space,\n",
              " over,\n",
              " the,\n",
              " next,\n",
              " three,\n",
              " years,\n",
              " ,,\n",
              " with,\n",
              " the,\n",
              " Republic,\n",
              " facing,\n",
              " a,\n",
              " huge,\n",
              " shortage,\n",
              " of,\n",
              " professionals,\n",
              " ,,\n",
              " said,\n",
              " Minister,\n",
              " -,\n",
              " in,\n",
              " -,\n",
              " charge,\n",
              " of,\n",
              " the,\n",
              " Smart,\n",
              " Nation,\n",
              " Initiative,\n",
              " Vivian,\n",
              " Balakrishnan,\n",
              " .,\n",
              " \n",
              " ,\n",
              " Singaporeans,\n",
              " can,\n",
              " secure,\n",
              " these,\n",
              " well,\n",
              " -,\n",
              " paying,\n",
              " jobs,\n",
              " if,\n",
              " they,\n",
              " are,\n",
              " prepared,\n",
              " to,\n",
              " train,\n",
              " and,\n",
              " reskill,\n",
              " ,,\n",
              " said,\n",
              " Dr,\n",
              " Balakrishnan,\n",
              " ,,\n",
              " who,\n",
              " is,\n",
              " also,\n",
              " Foreign,\n",
              " Minister,\n",
              " ,,\n",
              " in,\n",
              " an,\n",
              " interview,\n",
              " with,\n",
              " The,\n",
              " Straits,\n",
              " Times,\n",
              " .,\n",
              " \n",
              " ,\n",
              " He,\n",
              " said,\n",
              " digitalisation,\n",
              " and,\n",
              " disruption,\n",
              " had,\n",
              " already,\n",
              " taken,\n",
              " root,\n",
              " when,\n",
              " the,\n",
              " current,\n",
              " crisis,\n",
              " struck,\n",
              " .,\n",
              " Covid-19,\n",
              " accelerated,\n",
              " these,\n",
              " trends,\n",
              " .,\n",
              " \n",
              " ,\n",
              " While,\n",
              " Singapore,\n",
              " 's,\n",
              " immediate,\n",
              " priority,\n",
              " is,\n",
              " to,\n",
              " save,\n",
              " jobs,\n",
              " ,,\n",
              " that,\n",
              " is,\n",
              " not,\n",
              " enough,\n",
              " because,\n",
              " \",\n",
              " jobs,\n",
              " are,\n",
              " going,\n",
              " to,\n",
              " change,\n",
              " \",\n",
              " ,,\n",
              " he,\n",
              " added,\n",
              " .,\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px2Kb77FHWcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e71dc327-bd31-486b-9a3f-a5ddbb201172"
      },
      "source": [
        "# Taken from: https://www.zaobao.com.sg/zvideos/news/story20200621-1062143\n",
        "text_zh = \"\"\"\n",
        "今年的8月9日对林猷冠来说，将和往年大为不同。这位过去60年里每年都必定亲临现场为新加坡庆生的“最忠实出席者”，今年因国庆形式调整，首次无法到现场庆祝国庆。\n",
        "\n",
        "林猷冠（75岁）不只每年看国庆，早年还是在庆典上亮相的表演者，每年表演醒狮。\n",
        "\n",
        "70年代中期后，国庆庆典上的醒狮表演逐渐少了，林猷冠的身份也从表演者转为观众。但他对国庆的热忱不减，每年坚持到现场观看庆典。\n",
        "\"\"\"\n",
        "\n",
        "doc_zh = nlp_zh(text_zh) \n",
        "[t for t in doc_zh]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[,\n",
              " 今年,\n",
              " 的,\n",
              " 8月,\n",
              " 9日,\n",
              " 对,\n",
              " 林猷冠,\n",
              " 来说,\n",
              " ，,\n",
              " 将,\n",
              " 和,\n",
              " 往年,\n",
              " 大为,\n",
              " 不同,\n",
              " 。,\n",
              " 这位,\n",
              " 过去,\n",
              " 60年,\n",
              " 里,\n",
              " 每年,\n",
              " 都,\n",
              " 必定,\n",
              " 亲临,\n",
              " 现场,\n",
              " 为,\n",
              " 新加坡,\n",
              " 庆生,\n",
              " 的,\n",
              " “,\n",
              " 最,\n",
              " 忠实,\n",
              " 出席者,\n",
              " ”,\n",
              " ，,\n",
              " 今年,\n",
              " 因,\n",
              " 国庆,\n",
              " 形式,\n",
              " 调整,\n",
              " ，,\n",
              " 首次,\n",
              " 无法,\n",
              " 到,\n",
              " 现场,\n",
              " 庆祝,\n",
              " 国庆,\n",
              " 。,\n",
              " \n",
              " ,\n",
              " 林猷冠,\n",
              " （,\n",
              " 75,\n",
              " 岁,\n",
              " ）,\n",
              " 不只,\n",
              " 每年,\n",
              " 看,\n",
              " 国庆,\n",
              " ，,\n",
              " 早年,\n",
              " 还是,\n",
              " 在,\n",
              " 庆典,\n",
              " 上,\n",
              " 亮相,\n",
              " 的,\n",
              " 表演者,\n",
              " ，,\n",
              " 每年,\n",
              " 表演,\n",
              " 醒狮,\n",
              " 。,\n",
              " \n",
              " ,\n",
              " 70年代,\n",
              " 中期,\n",
              " 后,\n",
              " ，,\n",
              " 国庆,\n",
              " 庆典,\n",
              " 上,\n",
              " 的,\n",
              " 醒狮,\n",
              " 表演,\n",
              " 逐渐,\n",
              " 少,\n",
              " 了,\n",
              " ，,\n",
              " 林猷冠,\n",
              " 的,\n",
              " 身份,\n",
              " 也,\n",
              " 从,\n",
              " 表演者,\n",
              " 转为,\n",
              " 观众,\n",
              " 。,\n",
              " 但,\n",
              " 他,\n",
              " 对,\n",
              " 国庆,\n",
              " 的,\n",
              " 热忱不减,\n",
              " ，,\n",
              " 每年,\n",
              " 坚持,\n",
              " 到,\n",
              " 现场,\n",
              " 观看,\n",
              " 庆典,\n",
              " 。,\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88J9ZDJGTaxW",
        "colab_type": "text"
      },
      "source": [
        "### Clean the text\n",
        "\n",
        "Each document contains a list of tokens. We'll go through each of them and discard the tokens we don't need.\n",
        "\n",
        "Q: How do we decide what we need or don't need?\n",
        "\n",
        "A: This depends on what you are trying to build. For example, are punctuation or upper/lower-case important for understanding your text?\n",
        "\n",
        "Commonly done:\n",
        "- Remove punctuation\n",
        "- Remove extra spaces (such as newlines)\n",
        "- Remove very common words that don't add to meaning (aka. stop words)\n",
        "\n",
        "Note: Casing is automatically handled by spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVJkItsPT5RU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "942aa6b6-7014-4d33-e5c0-ab16804dcd56"
      },
      "source": [
        "# We will combine tokenisation and cleaning into a helper function so we\n",
        "# can see end-to-end\n",
        "def clean_text(text, nlp):\n",
        "  \"\"\"Takes in an input text and returns tokens after cleaning\n",
        "  \"\"\"\n",
        "  doc = nlp(text)\n",
        "  return [t for t in doc if not (t.is_punct \n",
        "    or t.is_space or t.is_stop)]\n",
        "\n",
        "clean_text(text_en, nlp_en)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Jobs,\n",
              " available,\n",
              " Singaporeans,\n",
              " infocommunications,\n",
              " space,\n",
              " years,\n",
              " Republic,\n",
              " facing,\n",
              " huge,\n",
              " shortage,\n",
              " professionals,\n",
              " said,\n",
              " Minister,\n",
              " charge,\n",
              " Smart,\n",
              " Nation,\n",
              " Initiative,\n",
              " Vivian,\n",
              " Balakrishnan,\n",
              " Singaporeans,\n",
              " secure,\n",
              " paying,\n",
              " jobs,\n",
              " prepared,\n",
              " train,\n",
              " reskill,\n",
              " said,\n",
              " Dr,\n",
              " Balakrishnan,\n",
              " Foreign,\n",
              " Minister,\n",
              " interview,\n",
              " Straits,\n",
              " Times,\n",
              " said,\n",
              " digitalisation,\n",
              " disruption,\n",
              " taken,\n",
              " root,\n",
              " current,\n",
              " crisis,\n",
              " struck,\n",
              " Covid-19,\n",
              " accelerated,\n",
              " trends,\n",
              " Singapore,\n",
              " immediate,\n",
              " priority,\n",
              " save,\n",
              " jobs,\n",
              " jobs,\n",
              " going,\n",
              " change,\n",
              " added]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8N3pzb3VPGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "outputId": "bbf67175-0f2f-4961-9744-efcd4c784f7b"
      },
      "source": [
        "clean_text(text_zh, nlp_zh)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8月,\n",
              " 9日,\n",
              " 林猷冠,\n",
              " 往年,\n",
              " 大为,\n",
              " 这位,\n",
              " 60年,\n",
              " 里,\n",
              " 亲临,\n",
              " 现场,\n",
              " 新加坡,\n",
              " 庆生,\n",
              " 忠实,\n",
              " 出席者,\n",
              " 国庆,\n",
              " 形式,\n",
              " 调整,\n",
              " 首次,\n",
              " 现场,\n",
              " 庆祝,\n",
              " 国庆,\n",
              " 林猷冠,\n",
              " 75,\n",
              " 岁,\n",
              " 国庆,\n",
              " 早年,\n",
              " 庆典,\n",
              " 亮相,\n",
              " 表演者,\n",
              " 表演,\n",
              " 醒狮,\n",
              " 70年代,\n",
              " 中期,\n",
              " 国庆,\n",
              " 庆典,\n",
              " 醒狮,\n",
              " 表演,\n",
              " 少,\n",
              " 林猷冠,\n",
              " 身份,\n",
              " 表演者,\n",
              " 转为,\n",
              " 观众,\n",
              " 国庆,\n",
              " 热忱不减,\n",
              " 现场,\n",
              " 观看,\n",
              " 庆典]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puzCArVVWlJX",
        "colab_type": "text"
      },
      "source": [
        "## Part 1b - Exploring Tokens\n",
        "What else can we do with tokens?\n",
        "\n",
        "Let's take a look at some attributes of a token: https://spacy.io/api/token#attributes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_jTEAZsXEnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's use the cleaned tokens\n",
        "tokens_en = clean_text(text_en, nlp_en)\n",
        "tokens_zh = clean_text(text_zh, nlp_zh)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uOgBnpjXt3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_token_info(token):\n",
        "  \"\"\"Gets some information about any token\"\"\"\n",
        "  print('Text:', token.text)\n",
        "  print('Part of Speech (name, id):', token.pos_, token.pos)\n",
        "  print('Syntatic dependency:', token.dep_)\n",
        "  print('Syntatic Ancestors:', list(token.ancestors))\n",
        "  print('Syntatic Children:', list(token.children))\n",
        "  print('Word Vector:', list(token.vector))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdesF-EVYRs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "bb0754f2-78b7-4a53-ec09-8a15204f9d59"
      },
      "source": [
        "get_token_info(tokens_en[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: Jobs\n",
            "Part of Speech (name, id): NOUN 92\n",
            "Syntatic dependency: nsubj\n",
            "Syntatic Ancestors: [are, said]\n",
            "Syntatic Children: [\n",
            "]\n",
            "Word Vector: [-0.40137, -0.19474, 0.19818, -0.54391, 0.5309, 0.21999, -0.32818, 0.17793, 0.016401, 2.8541, -0.72737, -0.16053, -0.36131, -0.23523, -0.22177, 0.057084, 0.35281, 1.3199, -0.057253, 0.13008, 0.41834, 0.13319, 0.0074174, -0.21275, -0.023246, 0.20783, -0.35368, 0.14472, 0.68741, 0.38607, -0.14863, 0.01487, -0.19167, -0.5062, 0.75318, 0.38715, -0.28412, 0.1147, -0.63108, -0.061117, 0.039542, -0.078562, 0.10637, -0.23889, -0.21867, -0.023955, 0.86638, -0.38597, 0.89935, -0.36541, -0.007828, 0.21034, 0.13718, -0.3254, -0.52749, 0.43597, 0.14639, 0.028763, 0.28184, -1.0619, -0.52786, -0.88469, -0.30208, 0.19786, 1.4698, 0.0067428, 0.30814, -0.051615, -0.096924, 0.43097, -0.19442, 0.13786, -0.32614, 0.052293, 0.18228, 0.87438, 0.30449, 0.075829, 0.72784, 0.30703, -0.023131, -0.16769, 0.28953, -0.68041, -0.43045, 0.32522, 0.25538, 1.0164, 0.3058, -0.55121, -0.35774, -0.21213, -0.17357, -0.25733, 0.3795, 0.14245, -0.14306, 0.0067099, -0.03196, -0.3945, -0.51138, -0.023619, -0.1748, 0.42499, 0.26489, -0.56952, -0.3306, -0.050024, -0.12753, 0.42436, 0.11279, -0.52238, -0.363, 0.17332, 0.81041, 0.031304, 0.44279, 0.14885, 0.011846, -0.42452, 0.24963, -0.053352, -0.054832, -0.74474, 0.059633, -0.20949, 0.072993, 0.25198, 0.40471, -0.17838, 0.40314, -0.32715, -0.12058, -0.37789, 0.367, -0.24727, -0.67594, -0.017063, -0.47778, 0.10693, 0.47294, 0.28899, 0.19776, -0.17696, -0.61112, 0.065152, -0.025838, 0.27856, 0.2691, -0.35514, -0.020296, -0.44099, -0.42204, -0.19784, 0.40887, -0.077042, 0.24079, 0.16272, -0.015732, -0.30781, 0.20439, 0.14926, -0.13345, -0.253, -0.27029, -0.1277, -0.15853, -1.1341, 0.41858, -0.092391, -0.509, 0.32582, -0.83564, -0.27087, 0.33336, 0.69663, -0.95239, 0.30663, 0.67671, -0.075326, 0.34065, -0.44855, -0.26065, 0.42341, 0.56772, 0.30301, 0.44444, -0.21, 0.11659, -0.21711, -0.12504, 0.14252, 0.41105, 0.24357, -0.024195, 0.018596, 0.3063, -0.33918, 0.072837, -0.60451, 0.085889, -0.51024, 0.69874, -0.30607, -0.14931, -0.24156, -0.060618, -0.087356, 0.62127, -0.13404, -0.0020968, -0.3274, 0.19177, 0.6276, 0.28945, -0.28445, 0.16366, -0.56529, 0.042418, -0.19638, -0.34776, 0.21262, -0.44288, 0.15795, 0.10789, 0.024975, -0.26859, 0.1038, -0.30082, -0.44872, -0.038764, 0.25481, 0.086887, 0.010651, -0.0119, -0.15031, 0.25395, -0.24904, -0.00031422, 0.039556, -0.1586, -0.89605, 0.24803, -0.034598, 0.24563, -0.71942, -0.014434, 0.099594, -0.2629, -0.16294, -0.37327, 0.31369, -0.19471, -0.79784, 0.020979, -0.10572, -0.30149, 0.60441, -0.24563, -0.16595, 0.19678, 0.76048, -0.22934, 0.26227, 0.06664, -0.71306, -0.03172, -0.36983, 0.35513, 0.022471, -0.49492, -0.18356, 0.42977, 0.02594, -0.5768, 0.046096, 0.096204, 0.20267, 0.68989, 0.10379, 0.0059252, 0.7066, -0.40729, 1.0941, 0.65034, -0.79977, 0.38452, -0.089542, -0.060221, -0.87802, 0.041848, 0.057174, 0.48853, -0.38654, 0.06156, 0.14147, -0.84896, 0.08216, -0.62672, -0.49653]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH7w1WAiZdHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "fa89a12d-53c2-492a-ec02-b01daf633139"
      },
      "source": [
        "get_token_info(tokens_zh[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: 8月\n",
            "Part of Speech (name, id): NOUN 92\n",
            "Syntatic dependency: compound:nn\n",
            "Syntatic Ancestors: [9日, 不同]\n",
            "Syntatic Children: []\n",
            "Word Vector: [-0.91128, -2.6509, 3.6976, -3.1713, 4.8117, -1.656, -2.3266, 7.2307, 4.6191, -0.097682, -5.0225, -0.11716, 1.1935, -0.51917, -4.3766, -0.18002, 1.6073, 4.9088, -2.6837, 4.4773, -4.6425, 1.2229, 1.7836, 0.44313, 2.4923, 6.7588, -3.1278, -5.6155, -3.2504, -0.2284, -1.2825, 0.84207, 1.527, -1.2087, 0.62729, -2.7056, -3.4955, -7.602, -1.3812, 1.4504, 1.766, -1.3787, 1.6272, 1.7121, -2.8224, 4.7295, -4.8305, -2.3925, -0.80655, -0.81372, 0.43209, -3.4676, 3.2186, 10.491, -4.0767, -2.7056, -0.14365, -0.55266, 0.39485, -1.4152, 4.2886, -1.2944, -2.3571, -1.7825, -0.7717, 2.238, 3.6974, 0.091701, -3.9523, 4.634, 0.09359, 1.0207, -2.8256, 3.4699, 0.022762, -3.5618, 2.2222, -1.9482, -2.0209, -1.3345, 0.36243, 4.3488, -3.4396, 1.4275, -5.5295, -2.5989, -1.752, 1.5052, -0.053689, -1.3154, -9.5545, -5.4593, 4.2067, 4.3516, 3.165, 8.8794, 0.73182, 2.911, 1.41, 3.5808, -1.6745, -2.4049, 1.7181, 2.251, 0.25499, 0.35631, 3.0264, 1.1089, 0.95764, -1.1336, 4.0679, 3.0949, -0.083279, 1.7318, -2.3882, 8.9708, 4.1554, 1.0764, -0.67769, 2.441, -1.1847, 2.1538, 2.1559, -0.95226, -1.9511, 0.5653, 4.9887, 4.8699, 2.0723, 2.959, 1.4428, -5.2034, 3.5686, 2.0365, -3.3738, -0.12631, 1.2541, 2.3565, 1.2968, -3.2466, 0.38803, 1.752, -4.5706, 2.6493, 3.1783, 2.5271, 2.2418, 8.6882, 2.1742, -3.4847, -2.5951, -1.6326, 3.118, -2.2989, 2.1624, 0.53351, 0.077791, 0.41375, -0.71654, -3.3772, -6.0078, -3.4029, -3.4025, -0.15977, 1.736, 3.7963, -6.3755, -1.426, -6.3204, 6.9104, -1.2921, -4.5975, -1.8224, -2.1867, 2.7083, 0.81096, -6.0615, 0.70142, 6.4619, -2.4786, -2.7216, -3.122, 3.0465, -5.8105, 1.2479, -1.1089, -0.94388, 3.1997, -1.4966, 4.42, -2.27, 0.63282, -2.4666, 2.8035, 0.44169, 9.9806, -2.0152, 0.93834, -1.5398, -1.4003, 0.63938, -1.7823, -6.1132, -0.39603, -3.5366, 2.2039, 3.015, 4.4471, 3.4708, 3.318, -1.7072, 0.24335, 5.462, -0.080476, 2.0168, -1.8762, -4.0414, -2.1857, 0.6681, -1.69, 0.46047, 3.2781, 2.9455, 0.077457, 4.9907, 2.9773, -1.0631, -2.4431, 2.4735, -7.7477, -3.3762, -3.101, -1.3317, -2.4163, 4.0811, 3.9431, -0.52306, 1.2758, 2.5218, -2.1915, -2.619, 0.28717, -4.133, -2.5371, 3.7831, -2.1731, -1.1646, 3.2889, -1.3609, -1.8641, 5.4529, 4.2795, -0.4582, -3.8937, 2.3818, -3.9874, -0.50441, 2.9178, 4.078, 2.3753, -4.589, 0.028897, -1.591, -5.3739, -3.8322, 2.3203, -7.0382, 3.1099, -2.4375, 3.5614, 3.9407, -2.5911, 4.3491, -2.1888, 3.4621, -3.4429, 0.82044, 3.8818, -1.3431, -4.3044, -0.14596, -1.0612, 1.8397, -1.5082, 4.6154, 0.97618, 1.0962, -0.061187, -4.4258, -0.47362, 2.6298, 1.4627, 2.1714, -2.935, -2.0217, 3.6116, -2.0959, 3.9741, 2.3962, -5.198]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41BMeCg2ZyRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "fe382c47-0d16-4d7f-dd36-0af718a92024"
      },
      "source": [
        "get_token_info(tokens_en[1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: available\n",
            "Part of Speech (name, id): ADJ 84\n",
            "Syntatic dependency: acomp\n",
            "Syntatic Ancestors: [are, said]\n",
            "Syntatic Children: [for]\n",
            "Word Vector: [0.014507, -0.11624, 0.010342, -0.14761, 0.1576, -0.14334, 0.091898, -0.24521, -0.026863, 1.583, 0.023671, 0.56386, 0.013103, -0.35812, 0.29182, 0.086263, -0.02164, 2.3166, -0.076147, -0.53473, -0.89396, -0.15404, -0.17019, 0.019818, -0.10969, 0.22048, -0.085441, 0.00033733, 0.16612, -0.017404, 0.014849, -0.57812, 0.19454, -0.0040787, -0.38443, 0.036314, -0.17783, 0.136, 0.21289, 0.3293, -0.2546, 0.29294, -0.12063, -0.05737, -0.26232, 0.45102, 0.15652, -0.12469, 0.38356, -0.13506, -0.042586, -0.63343, -0.039293, 0.30387, 0.063688, -0.26632, -0.026717, -0.55038, -0.63944, -0.12253, 0.16655, -0.33855, 0.080006, 0.45584, 0.38396, 0.37248, -0.52798, 0.36999, -0.020948, 0.44524, -0.076609, -0.0076453, 0.81858, -0.31565, 0.53618, 0.25676, -0.0041489, 0.16316, 0.033448, 0.43533, -0.10441, 0.20812, -0.35925, 0.16069, -0.027052, -0.035009, -0.35581, 0.50112, 0.63855, -0.20734, 0.029605, 0.36625, 0.13515, -0.12046, 0.14261, -0.20593, -0.035545, 0.15655, -0.026144, -0.17946, -0.34717, 0.035709, -0.0051285, 0.43291, 0.25572, -0.3136, -0.18686, 0.30298, -0.17403, -0.06549, 0.10109, -0.39247, 0.59386, 0.044101, 0.28328, -0.04048, 0.24536, 0.28002, 0.085857, 0.38434, -0.029564, 0.1782, 0.002701, -0.40793, 0.014738, -0.10276, 0.10377, -0.17447, 0.47794, -0.22322, 0.29548, -0.51727, 0.02994, -0.13321, 0.036382, 0.49976, -0.038916, -0.27944, 0.10477, 0.066212, 0.089264, 0.18465, 0.37138, -0.44888, 0.17666, -0.5547, 0.14372, -0.23891, 0.25838, -0.13543, -0.31553, -0.065803, -0.28975, -0.3381, -0.46142, 0.071756, -0.11711, 0.0049992, -0.035558, -0.017172, 0.14225, 0.053836, -0.036669, -0.76752, 0.1659, 0.091722, 0.24717, 0.021982, 0.27378, -0.34327, 0.0066341, -0.01485, 0.25668, 0.18737, -0.21675, 0.38742, 0.048649, -0.2917, 0.23336, 0.52671, 0.11948, 0.11275, 0.071636, -0.13227, -0.34866, -0.33272, -0.040038, -0.10106, 0.020197, 0.61629, -0.55023, 0.19182, 0.2243, 0.21753, -0.080913, 0.19254, -0.33462, 0.64496, 0.096261, 0.39701, 0.1968, -0.34774, 0.064455, -0.22183, 0.57437, 0.2206, -0.055076, 0.15888, -0.1863, 0.47205, 0.00193, -0.1974, 0.055705, -0.44887, 0.029601, 0.30282, 0.081396, 0.21129, -0.74025, 0.36318, -0.29849, -0.29003, 0.062487, -0.20938, 0.1703, 0.016421, 0.033108, -0.26561, 0.19147, 0.23661, 0.038555, -0.25777, 0.65766, -0.017141, 0.30305, -0.060576, 0.18006, -0.6189, 0.070895, 0.31909, 0.097771, 0.35595, -0.3881, -0.036091, 0.37846, -0.31005, -0.37436, 0.19838, -0.28532, 0.14652, 0.21537, -0.2963, -0.3622, -0.089261, 0.45414, 0.44746, 0.41071, -0.22855, -0.10752, -0.18022, 0.3601, -0.27987, -0.24764, 0.15602, -0.51254, 0.032316, 0.049243, 0.42272, 0.53384, -0.17769, 0.096599, 0.1342, 0.17903, -0.017986, -0.010457, -0.04995, 0.10951, 0.70733, 0.0091384, 0.1074, 0.063498, 0.18939, -0.15128, -0.30982, 0.19866, -0.10743, -0.019501, -0.087331, -0.053955, 0.076729, -0.71336, -0.23579, 0.036933, -0.10303, -0.017653, 0.032475, -0.053972, -0.79815, 0.16186, 0.41637]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDU3gGRXeoFH",
        "colab_type": "text"
      },
      "source": [
        "### Part of Speech\n",
        "Part of speech tags can be used as filters.\n",
        "\n",
        "For example, you can use the same pattern that clean_text() used to extract just the nouns.\n",
        "\n",
        "https://spacy.io/usage/linguistic-features#pos-tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zex21fIvZ5uN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "aa2cb2f1-64dc-4d7f-dd2c-605367d004da"
      },
      "source": [
        "def get_nouns(text, nlp):\n",
        "  \"\"\"Takes in an input text and returns only the nouns\n",
        "  \"\"\"\n",
        "  doc = nlp(text)\n",
        "  return [t for t in doc if t.pos_ == \"NOUN\"]\n",
        "\n",
        "get_nouns(text_en, nlp_en)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Jobs,\n",
              " infocommunications,\n",
              " space,\n",
              " years,\n",
              " shortage,\n",
              " professionals,\n",
              " charge,\n",
              " jobs,\n",
              " interview,\n",
              " digitalisation,\n",
              " disruption,\n",
              " root,\n",
              " crisis,\n",
              " trends,\n",
              " priority,\n",
              " jobs,\n",
              " jobs]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRXgLdcehXpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "76009f72-b2d7-4382-fae9-f0422d70f05b"
      },
      "source": [
        "# let's see how well this PoS tagging works\n",
        "tricky_text_en = \"\"\"\n",
        "She thinks fast when she had a fast. The correct beat must be beat.\n",
        "\"\"\" \n",
        "\n",
        "def get_pos(text, nlp):\n",
        "  doc = nlp(text)\n",
        "  return [(t.text, t.pos_) for t in doc]\n",
        "\n",
        "get_pos(tricky_text_en, nlp_en)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\\n', 'SPACE'),\n",
              " ('She', 'PRON'),\n",
              " ('thinks', 'VERB'),\n",
              " ('fast', 'ADV'),\n",
              " ('when', 'ADV'),\n",
              " ('she', 'PRON'),\n",
              " ('had', 'AUX'),\n",
              " ('a', 'DET'),\n",
              " ('fast', 'NOUN'),\n",
              " ('.', 'PUNCT'),\n",
              " ('The', 'DET'),\n",
              " ('correct', 'ADJ'),\n",
              " ('beat', 'NOUN'),\n",
              " ('must', 'VERB'),\n",
              " ('be', 'AUX'),\n",
              " ('beat', 'VERB'),\n",
              " ('.', 'PUNCT'),\n",
              " ('\\n', 'SPACE')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NN-VxKGgJcO",
        "colab_type": "text"
      },
      "source": [
        "## Part 1c - Exploring Entity Recognition\n",
        "\n",
        "Entity Recognition can be done, but at the document level. This does not require tokenisation because spaCy already processed the text for you.\n",
        "\n",
        "https://spacy.io/usage/linguistic-features#named-entities\n",
        "\n",
        "We'll just do a quick demo using spacy's visualiser (displacy).\n",
        "\n",
        "https://spacy.io/usage/visualizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSuH1WoGgm0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "f0a11636-df11-4dff-b00a-533b6cb4209b"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc_en, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>Jobs are available for \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Singaporeans\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " in the infocommunications space over \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the next three years\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", with the Republic facing a huge shortage of professionals, said Minister-in-charge of the Smart Nation Initiative \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Vivian Balakrishnan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ".</br></br>\n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Singaporeans\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " can secure these well-paying jobs if they are prepared to train and reskill, said \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dr Balakrishnan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", who is also Foreign Minister, in an interview with \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Straits Times\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</br></br>He said digitalisation and disruption had already taken root when the current crisis struck. Covid-19 accelerated these trends.</br></br>While \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Singapore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "'s immediate priority is to save jobs, that is not enough because &quot;jobs are going to change&quot;, he added.\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wX2XcUvhHnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "702cc1a1-191d-4b94-ff7e-4dafc08f2674"
      },
      "source": [
        "displacy.render(doc_zh, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    今年的8月9日\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "对\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    林猷冠\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "来说，将和往年大为不同。这位\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    过去60年\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "里每年都必定亲临现场为\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    新加坡\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "庆生的“最忠实出席者”，\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    今年\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "因国庆形式调整，首次无法到现场庆祝国庆。</br></br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    林猷冠\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "（\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    75岁\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "）不只每年看国庆，早年还是在庆典上亮相的表演者，每年表演醒狮。</br></br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    70年代中期\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "后，国庆庆典上的醒狮表演逐渐少了，\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    林猷冠\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "的身份也从表演者转为观众。但他对国庆的热忱不减，每年坚持到现场观看庆典。\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkNDPZOwjhAS",
        "colab_type": "text"
      },
      "source": [
        "The symbols above are documented here:\n",
        "\n",
        "https://spacy.io/api/annotation#named-entities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfyYCJ5XjpqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d90116a-7b19-431a-85bd-c45f003b33e1"
      },
      "source": [
        "# too lazy to look it up, ask spaCy!\n",
        "spacy.explain(\"NORP\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nationalities or religious or political groups'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UbXN0rPjub5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d3de3ca-742e-4969-8e43-d755cd8c5542"
      },
      "source": [
        "spacy.explain(\"GPE\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7sRDfDeXTnc",
        "colab_type": "text"
      },
      "source": [
        "## Intermission: Try with your own text!\n",
        "\n",
        "Common issues:\n",
        "- Use the correct language model for your text.\n",
        "- If you want to try another language model, you need to Restart the Runtime after loading it.\n",
        "\n",
        "### Back in 15 minutes ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-ph1_zfeeet",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - Word Meaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQh76ozGgLAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "text_processing101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSEp3B9tsGOEALZJrfi99Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}